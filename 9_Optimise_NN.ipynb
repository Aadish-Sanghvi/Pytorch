{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HOW TO DEAL WITH OVERFITTING?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add more data\n",
    "2. Reduce complexity of NN  \n",
    "3. Regularisation\n",
    "4. Dropouts\n",
    "5. Data Augmentation\n",
    "6. Batch Normalization \n",
    "7. Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout randomly deactivates neurons during training (like temporarily turning off brain cells), forcing the network to not rely too heavily on any single neuron. This prevents overfitting - like learning from multiple teachers instead of memorizing from just one.\n",
    "\n",
    "1. Applied on Hidden layers only.\n",
    "2. Applies after any activation function.\n",
    "3. This has regularisation effect.\n",
    "4. During evaluation dropouts are not used.\n",
    "\n",
    "use nn.Dropout(p=0.2) where p is the probability of deactivating a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch Normalization\n",
    "1. Normalises the input to each layer.\n",
    "2. Reduces the problem of vanishing/exploding gradients.\n",
    "3. Helps in faster convergence.\n",
    "4. Acts as a regulariser.\n",
    "5. Allows for higher learning rates.\n",
    "\n",
    "Applied to hidden layers only. After linear layer and before activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(64, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularisation controlled by hyperparameters.\n",
    "\n",
    "- Early Stopping\n",
    "1. Stops training when the validation loss stops improving.\n",
    "2. Prevents overfitting.\n",
    "3. Saves time and resources.\n",
    "\n",
    "Example:\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
